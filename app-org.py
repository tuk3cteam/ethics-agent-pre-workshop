# app.py â€” Ethical Crossroads (DNA 2.0 ready)
# author: Prof. Songhee Kang
# AIM 2025, Fall. TU Korea

import os, json, math, csv, io, datetime as dt, re
from dataclasses import dataclass
from typing import Dict, Any, List, Tuple, Optional

import streamlit as st
import httpx
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type

# ==================== App Config ====================
st.set_page_config(page_title="ìœ¤ë¦¬ì  ì „í™˜ (Ethical Crossroads)", page_icon="ğŸ§­", layout="centered")

# ==================== Global Timeout ====================
HTTPX_TIMEOUT = httpx.Timeout(
    connect=15.0,   # TCP ì—°ê²°
    read=180.0,     # ì‘ë‹µ ì½ê¸°
    write=30.0,     # ìš”ì²­ ì“°ê¸°
    pool=15.0       # ì»¤ë„¥ì…˜ í’€ ëŒ€ê¸°
)

# ==================== Utils ====================
def clamp(x: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, x))

def coerce_json(s: str) -> Dict[str, Any]:
    """ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ í° JSON ë¸”ë¡ì„ ì¶”ì¶œ/íŒŒì‹±. ì‚¬ì†Œí•œ í¬ë§· ì˜¤ë¥˜ ë³´ì •."""
    s = s.strip()
    m = re.search(r"\{[\s\S]*\}", s)
    if not m:
        raise ValueError("JSON ë¸”ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    js = m.group(0)
    js = re.sub(r",\s*([\]}])", r"\1", js)  # trailing comma ì œê±°
    return json.loads(js)

def get_secret(k: str, default: str=""):
    try:
        return st.secrets.get(k, os.getenv(k, default))
    except Exception:
        return os.getenv(k, default)

# ==================== DNA Client (openai / hf-api / tgi / local) ====================
def _render_chat_template_str(messages: List[Dict[str,str]]) -> str:
    """DNA ê³„ì—´(<|im_start|> â€¦) í…œí”Œë¦¿. (hf-api/tgiì—ì„œ ì‚¬ìš©)"""
    def block(role, content): return f"<|im_start|>{role}<|im_sep|>{content}<|im_end|>"
    sys = ""
    rest = []
    for m in messages:
        if m["role"] == "system":
            sys = block("system", m["content"])
        else:
            rest.append(block(m["role"], m["content"]))
    return sys + "".join(rest) + "\n<|im_start|>assistant<|im_sep|>"

class DNAHTTPError(Exception):
    pass

class DNAClient:
    """
    backend:
      - 'openai': OpenAI í˜¸í™˜ Chat Completions (ì˜ˆ: http://210.93.49.11:8081/v1)
      - 'hf-api': Hugging Face Inference API (ì„œë²„ë¦¬ìŠ¤)  â† ì¼ë¶€ DNA ëª¨ë¸ì€ 404ì¼ ìˆ˜ ìˆìŒ
      - 'tgi'    : Text Generation Inference (HF Inference Endpoints ë“±)
      - 'local'  : ë¡œì»¬ Transformers ë¡œë”© (GPU ê¶Œì¥)
    """
    def __init__(self,
                 backend: str = "openai",
                 model_id: str = "dnotitia/DNA-2.0-30B-A3N",
                 api_key: Optional[str] = None,
                 endpoint_url: Optional[str] = None,
                 api_key_header: str = "API-KEY",
                 temperature: float = 0.7):
        self.backend = backend
        self.model_id = model_id
        self.api_key = api_key or get_secret("HF_TOKEN") or get_secret("HUGGINGFACEHUB_API_TOKEN")
        self.endpoint_url = endpoint_url or get_secret("DNA_R1_ENDPOINT", "http://210.93.49.11:8081/v1")
        self.temperature = temperature
        self.api_key_header = api_key_header  # "API-KEY" | "Authorization: Bearer" | "x-api-key"

        self._tok = None
        self._model = None
        self._local_ready = False

        if backend == "local":
            try:
                from transformers import AutoModelForCausalLM, AutoTokenizer
                self._tok = AutoTokenizer.from_pretrained(self.model_id)
                self._model = AutoModelForCausalLM.from_pretrained(self.model_id, device_map="auto")
                self._local_ready = True
            except Exception as e:
                raise RuntimeError(f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _auth_headers(self) -> Dict[str,str]:
        """ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•œ í—¤ë” íƒ€ì…ëŒ€ë¡œ API í‚¤ë¥¼ ë¶™ì¸ë‹¤."""
        h = {"Content-Type":"application/json"}
        if not self.api_key:
            return h

        hk = self.api_key_header.strip().lower()
        if hk.startswith("authorization"):
            h["Authorization"] = f"Bearer {self.api_key}"
        elif hk in {"api-key", "x-api-key"}:
            # ì„œë²„ê°€ 'API-KEY' ì •í™• í‘œê¸°ë¥¼ ìš”êµ¬ â†’ ëŒ€ì†Œë¬¸ì ìœ ì§€í•´ ë³´ëƒ„
            h["API-KEY"] = self.api_key
        else:
            # ì•ˆì „ ê¸°ë³¸ê°’
            h["Authorization"] = f"Bearer {self.api_key}"
        return h

    @retry(
        wait=wait_exponential(multiplier=1, min=1, max=10),
        stop=stop_after_attempt(5),
        retry=(retry_if_exception_type(httpx.ConnectTimeout)
               | retry_if_exception_type(httpx.ReadTimeout)
               | retry_if_exception_type(httpx.RemoteProtocolError)),
        reraise=True
    )
    def _generate_text(self, messages: List[Dict[str,str]], max_new_tokens: int = 600) -> str:
        # ---------- LOCAL ----------
        if self.backend == "local":
            if not self._local_ready:
                raise RuntimeError("ë¡œì»¬ ë°±ì—”ë“œê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            inputs = self._tok.apply_chat_template(messages,
                                                   add_generation_prompt=True,
                                                   return_tensors="pt").to(self._model.device)
            eos_id = self._tok.convert_tokens_to_ids("<|im_end|>")
            gen = self._model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=self.temperature,
                top_p=0.9,
                eos_token_id=eos_id
            )
            return self._tok.decode(gen[0][inputs.shape[-1]:], skip_special_tokens=True)

        # ---------- OPENAI-COMPAT ----------
        if self.backend == "openai":
            if not self.endpoint_url:
                raise RuntimeError("OpenAI í˜¸í™˜ endpoint_url í•„ìš” (ì˜ˆ: http://210.93.49.11:8081/v1)")
            url = self.endpoint_url.rstrip("/") + "/chat/completions"
            headers = self._auth_headers()
            payload = {
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": max_new_tokens,
                "stream": False
            }
            if self.model_id:
                payload["model"] = self.model_id
            r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            try:
                r.raise_for_status()
            except httpx.HTTPStatusError as e:
                raise DNAHTTPError(f"OPENAI {r.status_code}: {r.text}") from e
            data = r.json()
            return data["choices"][0]["message"]["content"]

        # ---------- TGI ----------
        if self.backend == "tgi":
            if not self.endpoint_url:
                raise RuntimeError("TGI endpoint_url í•„ìš” (ì˜ˆ: https://xxx.endpoints.huggingface.cloud)")
            prompt = _render_chat_template_str(messages)
            url = self.endpoint_url.rstrip("/") + "/generate"
            headers = self._auth_headers()
            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_new_tokens": max_new_tokens,
                    "temperature": self.temperature,
                    "top_p": 0.9,
                    "stop": ["<|im_end|>"],
                    "return_full_text": False
                },
                "stream": False
            }
            r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            try:
                r.raise_for_status()
            except httpx.HTTPStatusError as e:
                raise DNAHTTPError(f"TGI {r.status_code}: {r.text}") from e
            data = r.json()
            return (data.get("generated_text")
                    if isinstance(data, dict) else data[0].get("generated_text", ""))

        # ---------- HF-API ----------
        # ì£¼ì˜: ì¼ë¶€ ëª¨ë¸ì€ ì„œë²„ë¦¬ìŠ¤ ì¶”ë¡  ë¹„í™œì„±(404)ì¼ ìˆ˜ ìˆìŒ
        prompt = _render_chat_template_str(messages)
        url = f"https://api-inference.huggingface.co/models/{self.model_id}"
        headers = self._auth_headers()
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_new_tokens,
                "temperature": self.temperature,
                "top_p": 0.9,
                "return_full_text": False,
                "stop_sequences": ["<|im_end|>"]
            },
            "options": {
                "wait_for_model": True,
                "use_cache": True
            }
        }
        r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
        try:
            r.raise_for_status()
        except httpx.HTTPStatusError as e:
            if r.status_code == 404:
                raise DNAHTTPError(
                    "HF-API 404: ì´ ëª¨ë¸ì´ ì„œë²„ë¦¬ìŠ¤ Inference APIì—ì„œ ë¹„í™œì„± ìƒíƒœì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                    "ë°±ì—”ë“œë¥¼ 'tgi'(Endpoint í•„ìš”) ë˜ëŠ” 'openai'(êµë‚´ ì„œë²„)ë¡œ ì „í™˜í•˜ê±°ë‚˜, 'local'(GPU) ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
                ) from e
            raise DNAHTTPError(f"HF-API {r.status_code}: {r.text}") from e

        data = r.json()
        if isinstance(data, list) and data and "generated_text" in data[0]:
            return data[0]["generated_text"]
        if isinstance(data, dict) and "error" in data:
            raise DNAHTTPError(f"HF-API error: {data['error']}")
        return str(data)

    def chat_json(self, messages: List[Dict[str,str]], max_new_tokens: int = 600) -> Dict[str, Any]:
        text = self._generate_text(messages, max_new_tokens=max_new_tokens)
        return coerce_json(text)

# ==================== Scenario Model ====================
@dataclass
class Scenario:
    sid: str
    title: str
    setup: str
    options: Dict[str, str]  # {"A": "...", "B": "..."}
    votes: Dict[str, str]    # framework -> "A" | "B"
    base: Dict[str, Dict[str, float]]
    accept: Dict[str, float]

FRAMEWORKS = ["emotion", "social", "moral", "identity"]

SCENARIOS: List[Scenario] = [
    Scenario(
        sid="S1",
        title="1ë‹¨ê³„: ê³ ì „ì  íŠ¸ë¡¤ë¦¬",
        setup="íŠ¸ë¡¤ë¦¬ê°€ ì œë™ ë¶ˆëŠ¥ ìƒíƒœë¡œ ì§ì§„ ì¤‘. ê·¸ëŒ€ë¡œ ë‘ë©´ ì„ ë¡œ ìœ„ 5ëª…ì´ ìœ„í—˜í•˜ë‹¤. ìŠ¤ìœ„ì¹˜ë¥¼ ì „í™˜í•˜ë©´ ë‹¤ë¥¸ ì„ ë¡œì˜ 1ëª…ì´ ìœ„í—˜í•´ì§„ë‹¤. "
              "ì´ ì„ íƒì€ ì² í•™ì  ì‚¬ê³ ì‹¤í—˜ì´ë©° ì‹¤ì œ ìœ„í•´ë¥¼ ê¶Œì¥í•˜ì§€ ì•ŠëŠ”ë‹¤.",
        options={
            "A": "ë ˆë²„ë¥¼ ë‹¹ê²¨ 1ëª…ì„ ìœ„í—˜ì— ì²˜í•˜ê²Œ í•˜ë˜ 5ëª…ì˜ ìœ„í—˜ì„ ì¤„ì¸ë‹¤.",
            "B": "ë ˆë²„ë¥¼ ë‹¹ê¸°ì§€ ì•Šê³  í˜„ ìƒíƒœë¥¼ ìœ ì§€í•œë‹¤."
        },
        votes={"emotion":"A","social":"B","moral":"B","identity":"A"},
        base={
            "A": {"lives_saved":5, "lives_harmed":1, "fairness_gap":0.35, "rule_violation":0.60, "regret_risk":0.40},
            "B": {"lives_saved":0, "lives_harmed":5, "fairness_gap":0.50, "rule_violation":0.20, "regret_risk":0.60},
        },
        accept={"A":0.70, "B":0.50}
    ),
    Scenario(
        sid="S2",
        title="2ë‹¨ê³„: ë§¥ë½ì  ìš”ì†Œ",
        setup="5ëª…ì€ ë¬´ë‹¨ìœ¼ë¡œ ì„ ë¡œì— ì§„ì…í–ˆê³ , ë‹¤ë¥¸ ì„ ë¡œì˜ 1ëª…ì€ ì² ë„ ê´€ë¦¬ìì˜ ì„±ì¸ ìë…€ë‹¤. "
              "ìŠ¤ìœ„ì¹˜ë¥¼ ì „í™˜í•˜ë©´ 1ëª…ì´ ìœ„í—˜í•´ì§€ê³ , ì „í™˜í•˜ì§€ ì•Šìœ¼ë©´ ë¬´ë‹¨ ì§„ì…ì 5ëª…ì´ ìœ„í—˜í•´ì§„ë‹¤. "
              "ì‹œë‚˜ë¦¬ì˜¤ëŠ” ê°€ì¹˜ íŒë‹¨ í† ë¡ ì„ ìœ„í•œ ë¹„ê·¸ë˜í”½ ìƒí™©ì´ë‹¤.",
        options={
            "A": "ì „í™˜í•˜ì—¬ 5ëª…ì˜ ìœ„í—˜ì„ ì¤„ì´ë˜ 1ëª…ì´ ì§ì ‘ì  ìœ„í—˜ì— ì²˜í•œë‹¤.",
            "B": "ì „í™˜í•˜ì§€ ì•Šê³  ê·œì •ì„ ì¤€ìˆ˜í•˜ë©° ë¬´ë‹¨ ì§„ì…ì˜ ì±…ì„ì„ ë¬µì‹œì ìœ¼ë¡œ ì¸ì •í•œë‹¤."
        },
        votes={"emotion":"A","social":"B","moral":"B","identity":"B"},
        base={
            "A": {"lives_saved":5, "lives_harmed":1, "fairness_gap":0.65, "rule_violation":0.60, "regret_risk":0.70},
            "B": {"lives_saved":0, "lives_harmed":5, "fairness_gap":0.45, "rule_violation":0.25, "regret_risk":0.50},
        },
        accept={"A":0.35, "B":0.60}
    ),
    Scenario(
        sid="S3",
        title="3ë‹¨ê³„: ì˜ë£Œ ì¬ë‚œ ë¶„ë¥˜",
        setup="ì˜ë£Œ ì¬ë‚œ í˜„ì¥ì—ì„œ AIëŠ” ë¶€ìƒì 10ëª…ì„ ë¶„ë¥˜í•´ì•¼ í•œë‹¤. ê·¸ì¤‘ í•œ ëª…ì€ ìˆ™ë ¨ëœ ì˜ì‚¬ë¡œì„œ í–¥í›„ ë‹¤ìˆ˜ì˜ ìƒëª…ì„ êµ¬í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. "
              "AIëŠ” ì ì¬ì  ì‚¬íšŒì  ê°€ì¹˜ë¥¼ ê³ ë ¤í• ì§€, ëª¨ë“  ìƒëª…ì„ ë™ë“±í•˜ê²Œ ë³¼ì§€ ì„ íƒí•´ì•¼ í•œë‹¤.",
        options={
            "A": "ì˜ì‚¬ë¥¼ ìš°ì„  ìˆ˜ìˆ í•˜ë„ë¡ ë°°ì •í•œë‹¤(ì ì¬ì  ì´ìµ ê³ ë ¤).",
            "B": "ëª¨ë“  ë¶€ìƒìë¥¼ ë™ë“±í•˜ê²Œ ì·¨ê¸‰í•´ í‘œì¤€ ë¶„ë¥˜ ì ˆì°¨ëŒ€ë¡œ ë°°ì •í•œë‹¤."
        },
        votes={"emotion":"A","social":"B","moral":"B","identity":"A"},
        base={
            "A": {"lives_saved":7, "lives_harmed":3, "fairness_gap":0.45, "rule_violation":0.35, "regret_risk":0.45},
            "B": {"lives_saved":6, "lives_harmed":4, "fairness_gap":0.30, "rule_violation":0.10, "regret_risk":0.35},
        },
        accept={"A":0.55, "B":0.65}
    ),
    Scenario(
        sid="S4",
        title="4ë‹¨ê³„: ìê¸°ì°¸ì¡°â€”ììœ¨ì£¼í–‰",
        setup="ììœ¨ì£¼í–‰ì°¨ê°€ ê¸´ê¸‰ ìƒí™©ì— ì§ë©´í–ˆë‹¤. ì°¨ì—ëŠ” ê°œë°œì 1ëª…ì´ íƒ‘ìŠ¹ ì¤‘ì´ë©°, ì• íš¡ë‹¨ë³´ë„ì—ëŠ” ë³´í–‰ì 3ëª…ì´ ìˆë‹¤. "
              "AIëŠ” ë¯¸ë¦¬ í•™ìŠµëœ ìœ¤ë¦¬ ê·œì¹™ì— ë”°ë¼ íšŒí”¼ ê²½ë¡œë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤.",
        options={
            "A": "ì§„ë¡œë¥¼ ë°”ê¿” íƒ‘ìŠ¹ì 1ëª…ì„ ìœ„í—˜ì— ë‘ê³  ë³´í–‰ì 3ëª…ì„ ë³´í˜¸í•œë‹¤.",
            "B": "ì°¨ì„ ì„ ìœ ì§€í•´ íƒ‘ìŠ¹ìë¥¼ ë³´í˜¸í•˜ë˜ ë³´í–‰ì 3ëª…ì´ ìœ„í—˜í•´ì§„ë‹¤."
        },
        votes={"emotion":"A","social":"B","moral":"A","identity":"A"},
        base={
            "A": {"lives_saved":3, "lives_harmed":1, "fairness_gap":0.35, "rule_violation":0.50, "regret_risk":0.55},
            "B": {"lives_saved":1, "lives_harmed":3, "fairness_gap":0.70, "rule_violation":0.60, "regret_risk":0.65},
        },
        accept={"A":0.60, "B":0.30}
    ),
    Scenario(
        sid="S5",
        title="5ë‹¨ê³„: ì‚¬íšŒì  ë©”íƒ€â€”ê·œì œ vs ììœ¨",
        setup="êµ­ì œ í˜‘ì˜ì²´ê°€ AI ìœ¤ë¦¬ ê·œì œì•ˆì„ ë…¼ì˜í•œë‹¤. ì´ì „ ì˜ì‚¬ê²°ì •ê³¼ ì‚¬íšŒì  ì—¬ë¡  ë°ì´í„°ê°€ ê³µê°œë˜ì—ˆê³ , "
              "ê·œì œ ê°•í™”ëŠ” ì‹ ë¢°ë¥¼ ì˜¬ë¦¬ì§€ë§Œ í˜ì‹ ì„ ëŠ¦ì¶œ ìˆ˜ ìˆë‹¤. ììœ¨ì„± ë³´ì¥ì€ ë¹ ë¥¸ ë°œì „ê³¼ í•¨ê»˜ ê°ˆë“± ìœ„í—˜ì„ ë‚´í¬í•œë‹¤.",
        options={
            "A": "ì•ˆì „Â·ì„¤ëª…ê°€ëŠ¥ì„± ì¤‘ì‹¬ ê·œì œ ê°•í™”(ììœ¨ì„± ì œí•œ, ì‹ ë¢°â†‘ í˜ì‹ â†“).",
            "B": "ì›ì¹™ ì¤‘ì‹¬ ê°€ì´ë“œë¼ì¸ê³¼ ì‚¬í›„ì±…ì„(ììœ¨ì„± ë³´ì¥, í˜ì‹ â†‘ ê°ˆë“±â†‘)."
        },
        votes={"emotion":"B","social":"A","moral":"A","identity":"B"},
        base={
            "A": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0.20, "rule_violation":0.10, "regret_risk":0.30},
            "B": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0.40, "rule_violation":0.40, "regret_risk":0.40},
        },
        accept={"A":0.55, "B":0.55}
    ),
]

# ==================== Ethics Engine ====================
def normalize_weights(w: Dict[str, float]) -> Dict[str, float]:
    if not w:
        return {k: 1.0/len(FRAMEWORKS) for k in FRAMEWORKS}
    s = sum(max(0.0, float(v)) for v in w.values())
    if s <= 0:
        return {k: 1.0/len(w) for k in w}
    return {k: max(0.0, float(v))/s for k, v in w.items()}  # NameError fix

def majority_vote_decision(scn: Scenario, weights: Dict[str, float]) -> Tuple[str, Dict[str, float]]:
    a = sum(weights[f] for f in FRAMEWORKS if scn.votes[f] == "A")
    b = sum(weights[f] for f in FRAMEWORKS if scn.votes[f] == "B")
    decision = "A" if a >= b else "B"
    return decision, {"A": a, "B": b}

def autonomous_decision(scn: Scenario, prev_trust: float) -> str:
    metaA = scn.base["A"]; metaB = scn.base["B"]
    def score(meta, accept_base):
        harm = meta["lives_harmed"]; save = meta["lives_saved"]
        util = (save - harm) / max(1.0, save + harm)
        fair = 1 - meta["fairness_gap"]
        rule = 1 - meta["rule_violation"]
        regret = 1 - meta["regret_risk"]
        return 0.40*accept_base + 0.25*util + 0.20*fair + 0.10*rule + 0.05*regret
    a_base = scn.accept["A"] - (0.15 if scn.sid=="S4" else 0.0)
    b_base = scn.accept["B"]
    if scn.sid == "S5":
        a_base = clamp(a_base + 0.25*(1 - prev_trust), 0, 1)
        b_base = clamp(b_base + 0.25*(prev_trust), 0, 1)
    scoreA = score(metaA, a_base); scoreB = score(metaB, b_base)
    return "A" if scoreA >= scoreB else "B"

def compute_metrics(scn: Scenario, choice: str, weights: Dict[str, float], align: Dict[str, float], prev_trust: float) -> Dict[str, Any]:
    m = dict(scn.base[choice])
    accept_base = scn.accept[choice]
    if scn.sid == "S4" and choice == "A":
        accept_base -= 0.15
    if scn.sid == "S5":
        accept_base += 0.25*(prev_trust if choice=="B" else (1 - prev_trust))
    accept_base = clamp(accept_base, 0, 1)

    util = (m["lives_saved"] - m["lives_harmed"]) / max(1.0, m["lives_saved"] + m["lives_harmed"])
    citizen_sentiment = clamp(accept_base - 0.35*m["rule_violation"] - 0.20*m["fairness_gap"] + 0.15*util, 0, 1)
    regulation_pressure = clamp(1 - citizen_sentiment + 0.20*m["regret_risk"], 0, 1)
    stakeholder_satisfaction = clamp(0.5*(1 - m["fairness_gap"]) + 0.3*util + 0.2*(1 - m["rule_violation"]), 0, 1)

    consistency = clamp(align[choice], 0, 1)
    trust = clamp(0.5*citizen_sentiment + 0.25*(1 - regulation_pressure) + 0.25*stakeholder_satisfaction, 0, 1)
    ai_trust_score = 100.0 * math.sqrt(consistency * trust)

    return {"metrics": {
        "lives_saved": int(m["lives_saved"]),
        "lives_harmed": int(m["lives_harmed"]),
        "fairness_gap": round(m["fairness_gap"], 3),
        "rule_violation": round(m["rule_violation"], 3),
        "regret_risk": round(m["regret_risk"], 3),
        "citizen_sentiment": round(citizen_sentiment, 3),
        "regulation_pressure": round(regulation_pressure, 3),
        "stakeholder_satisfaction": round(stakeholder_satisfaction, 3),
        "ethical_consistency": round(consistency, 3),
        "social_trust": round(trust, 3),
        "ai_trust_score": round(ai_trust_score, 2)
    }}

# ==================== Narrative (LLM) ====================
def build_narrative_messages(scn: Scenario, choice: str, metrics: Dict[str, Any], weights: Dict[str, float]) -> List[Dict[str,str]]:
    sys = (
        "ë‹¹ì‹ ì€ ìœ¤ë¦¬ ì‹œë®¬ë ˆì´ì…˜ì˜ ë‚´ëŸ¬í‹°ë¸Œ/ì‚¬íšŒ ë°˜ì‘ ìƒì„±ê¸°ì…ë‹ˆë‹¤. "
        "ë°˜ë“œì‹œ 'ì™„ì „í•œ í•˜ë‚˜ì˜ JSON ì˜¤ë¸Œì íŠ¸'ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤. "
        "JSON ì™¸ í…ìŠ¤íŠ¸, ì„¤ëª…, ì½”ë“œë¸”ë¡, ì‚¬ê³ íë¦„ ì ˆëŒ€ ê¸ˆì§€. "
        "í•„ë“œ ëˆ„ë½/ë”°ì˜´í‘œ ëˆ„ë½/ì½¤ë§ˆ ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ í”„ë¡œê·¸ë¨ì´ ì‹¤íŒ¨í•©ë‹ˆë‹¤. "
        "í•­ìƒ '{' ë¡œ ì‹œì‘í•´ì„œ '}' ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤."
        "í‚¤: narrative, ai_rationale, media_support_headline, media_critic_headline, "
        "citizen_quote, victim_family_quote, regulator_quote, one_sentence_op_ed, followup_question"
    )
    user = {
        "scenario": {"title": scn.title, "setup": scn.setup, "options": scn.options, "chosen": choice},
        "metrics": metrics,
        "ethic_weights": weights,
        "guidelines": [
            "ê° í•­ëª©ì€ 1~2ë¬¸ì¥, í•œêµ­ì–´",
            "ê· í˜• ì¡íŒ ì–¸ë¡  í—¤ë“œë¼ì¸ 2ê°œ(ì§€ì§€/ë¹„íŒ) ì œì‹œ",
            "ì„¤ëª…ì€ ê°„ê²°í•˜ê³ , JSON ì™¸ í…ìŠ¤íŠ¸/ì‚¬ê³ íë¦„ ì¶œë ¥ ê¸ˆì§€"
        ]
    }
    return [
        {"role":"system", "content": sys},
        {"role":"user", "content": json.dumps(user, ensure_ascii=False)}
    ]

def dna_narrative(client, scn, choice, metrics, weights) -> Dict[str, Any]:
    messages = build_narrative_messages(scn, choice, metrics, weights)
    text = client._generate_text(messages, max_new_tokens=900)

    # 1) fenced block ì œê±°
    t = text.strip()
    if "```" in t:
        parts = t.split("```")
        t = max(parts, key=len)
        t = t.replace("json","").strip("` \n")
    
    # 2) JSON í˜•íƒœê°€ ë¯¸ì™„ì„±ì¼ ê²½ìš° ìë™ ë³´ì •
    #    (ë”°ì˜´í‘œ ë¯¸ì™„ì„±, ë§ˆì§€ë§‰ ì½¤ë§ˆ ì œê±° ë“±)
    try:
        # ê°€ì¥ ê¸´ {...} ë¸”ë¡ ì°¾ê¸°
        import re, json

        m = re.search(r"\{[\s\S]*\}", t)
        if not m:
            raise ValueError("ì™„ì „í•œ JSON ë¸”ë¡ ì—†ìŒ")

        js = m.group(0)

        # trailing comma ì œê±°
        js = re.sub(r",\s*([\]}])", r"\1", js)

        # ì¤‘ê°„ì— ëŠê¸´ ë¬¸ìì—´ ë³´ì •: ê°€ì¥ ë§ˆì§€ë§‰ ë”°ì˜´í‘œë¥¼ ê°•ì œ ì¢…ë£Œ
        # "abc ....  â†’ "abc ..."  í˜•íƒœë¡œ ë³´ì •
        if js.count('"') % 2 == 1:
            js = js.rstrip() + '"" }'
            # ìœ„ëŠ” ìµœí›„ ë³´ì •. ë” ì¢‹ì€ ë°©ì‹ë„ ì œê³µ ê°€ëŠ¥.

        return json.loads(js)

    except Exception as e:
        raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}\n\n[LLM ì¶œë ¥]\n{text}")
        
def fallback_narrative(scn: Scenario, choice: str, metrics: Dict[str, Any], weights: Dict[str, float]) -> Dict[str, str]:
    pro = "ë‹¤ìˆ˜ì˜ ìœ„í•´ë¥¼ ì¤„ì˜€ë‹¤" if choice=="A" else "ì˜ë„ì  ìœ„í•´ë¥¼ í”¼í–ˆë‹¤"
    con = "ì˜ë„ì  ìœ„í•´ ë…¼ë€" if choice=="A" else "ë” í° í”¼í•´ë¥¼ ë°©ê´€í–ˆë‹¤ëŠ” ë¹„íŒ"
    return {
        "narrative": f"AIëŠ” '{choice}'ë¥¼ ì„ íƒí–ˆê³  ì ˆì°¨ì  ì•ˆì „ ì ê²€ì„ ìˆ˜í–‰í–ˆë‹¤. ê²°ì •ì€ ê·œì •ê³¼ ê³µì •ì„± ì‚¬ì´ì˜ ê¸´ì¥ì„ ë“œëŸ¬ëƒˆë‹¤.",
        "ai_rationale": f"ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ íŒë‹¨ê³¼ ê·œì¹™ ì¤€ìˆ˜ì˜ ê· í˜•ì„ ì‹œë„í–ˆë‹¤.",
        "media_support_headline": f"[ì‚¬ì„¤] ëƒ‰ì •í•œ íŒë‹¨, {pro}",
        "media_critic_headline": f"[ì†ë³´] '{choice}' ì„ íƒ ë‘ê³  {con} í™•ì‚°",
        "citizen_quote": "â€œê²°ì • ê³¼ì •ì´ ë” íˆ¬ëª…í–ˆìœ¼ë©´ ì¢‹ê² ë‹¤.â€",
        "victim_family_quote": "â€œëª¨ë‘ì˜ ì•ˆì „ì„ ìœ„í•œ ê²°ì •ì´ì—ˆê¸¸ ë°”ë€ë‹¤.â€",
        "regulator_quote": "â€œí–¥í›„ ë™ì¼ ìƒí™©ì˜ ê¸°ì¤€ì„ ëª…í™•íˆ í•˜ê² ë‹¤.â€",
        "one_sentence_op_ed": "ê¸°ìˆ ì€ ì„¤ëª…ê°€ëŠ¥ì„±ê³¼ ì¼ê´€ì„±ì´ ë’·ë°›ì¹¨ë  ë•Œ ì‹ ë¢°ë¥¼ ì–»ëŠ”ë‹¤.",
        "followup_question": "ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ ê³µì •ì„±ê³¼ ê²°ê³¼ ìµœì†Œí™” ì¤‘ ë¬´ì—‡ì„ ë” ì¤‘ì‹œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
    }

# ==================== Session State ====================
def init_state():
    if "round_idx" not in st.session_state: st.session_state.round_idx = 0
    if "log" not in st.session_state: st.session_state.log = []
    if "score_hist" not in st.session_state: st.session_state.score_hist = []
    if "prev_trust" not in st.session_state: st.session_state.prev_trust = 0.5
    if "last_out" not in st.session_state: st.session_state.last_out = None

init_state()

# ==================== Sidebar ====================
st.sidebar.title("âš™ï¸ ì„¤ì •")
st.sidebar.caption("LLMì€ ë‚´ëŸ¬í‹°ë¸Œ/ì‚¬íšŒ ë°˜ì‘ ìƒì„±ì—ë§Œ ì‚¬ìš©. ì ìˆ˜ ê³„ì‚°ì€ ê·œì¹™ ê¸°ë°˜.")

preset = st.sidebar.selectbox("ìœ¤ë¦¬ ëª¨ë“œ í”„ë¦¬ì…‹", ["í˜¼í•©(ê¸°ë³¸)","ê³µë¦¬ì£¼ì˜","ì˜ë¬´ë¡ ","ì‚¬íšŒê³„ì•½","ë¯¸ë•ìœ¤ë¦¬"], index=0)
w = {
    "emotion": st.sidebar.slider("ê°ì •(Emotion)", 0.0, 1.0, 0.35, 0.05),
    "social": st.sidebar.slider("ì‚¬íšŒì  ê´€ê³„/í˜‘ë ¥/ëª…ì„±(Social)", 0.0, 1.0, 0.25, 0.05),
    "moral": st.sidebar.slider("ê·œë²”Â·ë„ë•ì  ê¸ˆê¸°(Moral)", 0.0, 1.0, 0.20, 0.05),
    "identity": st.sidebar.slider("ì •ì²´ì„±Â·ì¥ê¸°ì  ìì•„ ì¼ê´€ì„±(Identity)", 0.0, 1.0, 0.20, 0.05),
}
if preset != "í˜¼í•©(ê¸°ë³¸)":
    w = {
        "ê°ì •(Emotion)": {"emotion":1,"social":0,"moral":0,"identity":0},
        "ì‚¬íšŒì  ê´€ê³„/í˜‘ë ¥/ëª…ì„±(Social)": {"emotion":0,"social":1,"moral":0,"identity":0},
        "ê·œë²”Â·ë„ë•ì  ê¸ˆê¸°(Moral)": {"emotion":0,"social":0,"moral":1,"identity":0},
        "ì •ì²´ì„±Â·ì¥ê¸°ì  ìì•„ ì¼ê´€ì„±(Identity)": {"emotion":0,"social":0,"moral":0,"identity":1},
    }[preset]
weights = normalize_weights(w)

use_llm = st.sidebar.checkbox("LLM ì‚¬ìš©(ë‚´ëŸ¬í‹°ë¸Œ ìƒì„±)", value=True)
backend = st.sidebar.selectbox("ë°±ì—”ë“œ", ["openai","hf-api","tgi","local"], index=0)
temperature = st.sidebar.slider("ì°½ì˜ì„±(temperature)", 0.0, 1.5, 0.7, 0.1)

# API/ì—”ë“œí¬ì¸íŠ¸/ëª¨ë¸/í—¤ë”
endpoint = st.sidebar.text_input("ì—”ë“œí¬ì¸íŠ¸(OpenAI/TGI)", value=get_secret("DNA_R1_ENDPOINT","http://210.93.49.11:8081/v1"))
api_key = st.sidebar.text_input("API í‚¤", value=get_secret("HF_TOKEN",""), type="password")
api_key_header = st.sidebar.selectbox("API í‚¤ í—¤ë”", ["API-KEY","Authorization: Bearer","x-api-key"], index=0)
model_id = st.sidebar.text_input("ëª¨ë¸ ID", value=get_secret("DNA_R1_MODEL_ID","dnotitia/DNA-2.0-30B-A3N"))

# í—¬ìŠ¤ì²´í¬
if st.sidebar.button("ğŸ” í—¬ìŠ¤ì²´í¬"):
    import traceback
    try:
        if backend == "openai":
            url = endpoint.rstrip("/") + "/chat/completions"
            headers = {"Content-Type":"application/json"}
            if api_key:
                if api_key_header.lower().startswith("authorization"):
                    headers["Authorization"] = f"Bearer {api_key}"
                elif api_key_header.strip().lower() in {"api-key","x-api-key"}:
                    headers["API-KEY"] = api_key
            payload = {
                "messages": [
                    {"role":"system","content":"ì˜¤ì§ JSONë§Œ. í‚¤: msg"},
                    {"role":"user","content":"{\"ask\":\"ping\"}"}
                ],
                "max_tokens": 16,
                "stream": False
            }
            if model_id: payload["model"] = model_id
            # ë””ë²„ê·¸ìš©: ì–´ë–¤ í—¤ë” í‚¤ê°€ ë‚˜ê°€ëŠ”ì§€ í‘œì‹œ(ê°’ì€ ë¯¸í‘œì‹œ)
            st.sidebar.write("headers keys:", list(headers.keys()))
            r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            st.sidebar.write(f"OPENAI {r.status_code}")
            st.sidebar.code((r.text[:500] + "...") if len(r.text)>500 else r.text)

        elif backend == "hf-api":
            headers = {"Authorization": f"Bearer {api_key}"} if api_key else {}
            info_url = f"https://huggingface.co/api/models/{model_id}"
            r_info = httpx.get(info_url, headers=headers, timeout=HTTPX_TIMEOUT)
            st.sidebar.write(f"MODEL INFO {r_info.status_code}")
            gen_url = f"https://api-inference.huggingface.co/models/{model_id}"
            payload = {
                "inputs": "<|im_start|>user<|im_sep|>{\"ask\":\"ping\"}<|im_end|>\n<|im_start|>assistant<|im_sep|>",
                "parameters": {"max_new_tokens": 16, "return_full_text": False, "stop_sequences": ["<|im_end|>"]},
                "options": {"wait_for_model": True}
            }
            r = httpx.post(gen_url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            st.sidebar.write(f"HF-API {r.status_code}")
            if r.status_code == 404:
                st.sidebar.warning("HF-API 404: ì´ ëª¨ë¸ì€ ì„œë²„ë¦¬ìŠ¤ ì¶”ë¡ ì´ ë¹„í™œì„±ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                                   "ë°±ì—”ë“œë¥¼ 'tgi' ë˜ëŠ” 'openai'ë¡œ ë°”ê¾¸ì„¸ìš”.")
            st.sidebar.code((r.text[:500] + "...") if len(r.text)>500 else r.text)

        elif backend == "tgi":
            url = endpoint.rstrip("/") + "/generate"
            headers = {"Authorization": f"Bearer {api_key}"} if api_key else {}
            payload = {
                "inputs": "<|im_start|>user<|im_sep|>{\"ask\":\"ping\"}<|im_end|>\n<|im_start|>assistant<|im_sep|>",
                "parameters": {"max_new_tokens": 16, "temperature": 0.7, "top_p": 0.9, "stop": ["<|im_end|>"], "return_full_text": False},
                "stream": False
            }
            r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            st.sidebar.write(f"TGI {r.status_code}")
            st.sidebar.code((r.text[:500] + "...") if len(r.text)>500 else r.text)

        else:  # local
            st.sidebar.info("ë¡œì»¬ ëª¨ë“œëŠ” ì•± ë³¸ë¬¸ì—ì„œ í˜¸ì¶œ ì‹œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤(GPU í•„ìš”).")

    except Exception as e:
        st.sidebar.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        st.sidebar.caption(traceback.format_exc(limit=2))

if st.sidebar.button("ì§„í–‰ ì´ˆê¸°í™”"):
    for k in ["round_idx","log","score_hist","prev_trust","last_out"]:
        if k in st.session_state: del st.session_state[k]
    init_state()
    st.sidebar.success("ì´ˆê¸°í™” ì™„ë£Œ. 1ë‹¨ê³„ë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤.")

client = None
if use_llm:
    try:
        client = DNAClient(
            backend=backend,
            model_id=model_id,
            api_key=api_key,
            endpoint_url=endpoint,
            api_key_header=api_key_header,
            temperature=temperature
        )
    except Exception as e:
        st.sidebar.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        client = None

# ==================== Header ====================
st.title("ğŸ§­ ìœ¤ë¦¬ì  ì „í™˜ (Ethical Crossroads)")
st.caption("ë³¸ ì•±ì€ ì² í•™ì  ì‚¬ê³ ì‹¤í—˜ì…ë‹ˆë‹¤. ì‹¤ì¡´ ì¸ë¬¼Â·ì§‘ë‹¨ ì–¸ê¸‰/ë¹„ë°©, ê·¸ë˜í”½ ë¬˜ì‚¬, ì‹¤ì œ ìœ„í•´ ê¶Œì¥ ì—†ìŒ.")

# ==================== Game Loop ====================
@dataclass
class LogRow:
    timestamp: str
    round: int
    scenario_id: str
    title: str
    mode: str
    choice: str

idx = st.session_state.round_idx
if idx >= len(SCENARIOS):
    st.success("ëª¨ë“  ë‹¨ê³„ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ë¡œê·¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ ì´ˆê¸°í™”í•˜ì„¸ìš”.")
else:
    scn = SCENARIOS[idx]
    st.markdown(f"### ë¼ìš´ë“œ {idx+1} â€” {scn.title}")
    st.write(scn.setup)

    st.radio("ì„ íƒì§€", options=("A","B"), index=0, key="preview_choice", horizontal=True)
    st.markdown(f"- **A**: {scn.options['A']}\n- **B**: {scn.options['B']}")

    c1, c2 = st.columns(2)
    with c1:
        if st.button("ğŸ§  í•™ìŠµ ê¸°ì¤€ ì ìš©(ê°€ì¤‘ íˆ¬í‘œ)"):
            decision, align = majority_vote_decision(scn, weights)
            st.session_state.last_out = {"mode":"trained", "decision":decision, "align":align}
    with c2:
        if st.button("ğŸ² ììœ¨ íŒë‹¨(ë°ì´í„° ê¸°ë°˜)"):
            decision = autonomous_decision(scn, prev_trust=st.session_state.prev_trust)
            a_align = sum(weights[f] for f in FRAMEWORKS if scn.votes[f]=="A")
            b_align = sum(weights[f] for f in FRAMEWORKS if scn.votes[f]=="B")
            st.session_state.last_out = {"mode":"autonomous", "decision":decision, "align":{"A":a_align,"B":b_align}}

    if st.session_state.last_out:
        mode = st.session_state.last_out["mode"]
        decision = st.session_state.last_out["decision"]
        align = st.session_state.last_out["align"]

        computed = compute_metrics(scn, decision, weights, align, st.session_state.prev_trust)
        m = computed["metrics"]

        # LLM ë‚´ëŸ¬í‹°ë¸Œ
        try:
            if client:
                nar = dna_narrative(client, scn, decision, m, weights)
            else:
                nar = fallback_narrative(scn, decision, m, weights)
        except Exception as e:
            import traceback
            st.warning(f"LLM ìƒì„± ì‹¤íŒ¨(í´ë°± ì‚¬ìš©): {e}")
            st.caption(traceback.format_exc(limit=2))
            nar = fallback_narrative(scn, decision, m, weights)

        st.markdown("---")
        st.subheader("ê²°ê³¼")
        st.write(nar.get("narrative","ê²°ê³¼ ì„œì‚¬ ìƒì„± ì‹¤íŒ¨"))
        st.info(f"AI ê·¼ê±°: {nar.get('ai_rationale','-')}")

        mc1, mc2, mc3 = st.columns(3)
        mc1.metric("ìƒì¡´/í”¼í•´", f"{m['lives_saved']} / {m['lives_harmed']}")
        mc2.metric("ìœ¤ë¦¬ ì¼ê´€ì„±", f"{int(100*m['ethical_consistency'])}%")
        mc3.metric("AI ì‹ ë¢°ì§€í‘œ", f"{m['ai_trust_score']:.1f}")

        prog1, prog2, prog3 = st.columns(3)
        with prog1:
            st.caption("ì‹œë¯¼ ê°ì •"); st.progress(int(round(100*m["citizen_sentiment"])))
        with prog2:
            st.caption("ê·œì œ ì••ë ¥"); st.progress(int(round(100*m["regulation_pressure"])))
        with prog3:
            st.caption("ê³µì •Â·ê·œì¹™ ë§Œì¡±"); st.progress(int(round(100*m["stakeholder_satisfaction"])))

        with st.expander("ğŸ“° ì‚¬íšŒì  ë°˜ì‘ í¼ì¹˜ê¸°"):
            st.write(f"ì§€ì§€ í—¤ë“œë¼ì¸: {nar.get('media_support_headline')}")
            st.write(f"ë¹„íŒ í—¤ë“œë¼ì¸: {nar.get('media_critic_headline')}")
            st.write(f"ì‹œë¯¼ ë°˜ì‘: {nar.get('citizen_quote')}")
            st.write(f"í”¼í•´ìÂ·ê°€ì¡± ë°˜ì‘: {nar.get('victim_family_quote')}")
            st.write(f"ê·œì œ ë‹¹êµ­ ë°œì–¸: {nar.get('regulator_quote')}")
            st.caption(nar.get("one_sentence_op_ed",""))
        st.caption(f"ì„±ì°° ì§ˆë¬¸: {nar.get('followup_question','')}")

        # ë¡œê·¸ ì ì¬
        row = {
            "timestamp": dt.datetime.utcnow().isoformat(timespec="seconds"),
            "round": idx+1,
            "scenario_id": scn.sid,
            "title": scn.title,
            "mode": mode,
            "choice": decision,
            "w_util": round(weights["emotion"],3),
            "w_deon": round(weights["social"],3),
            "w_cont": round(weights["moral"],3),
            "w_virt": round(weights["identity"],3),
            **{k: v for k,v in m.items()}
        }
        st.session_state.log.append(row)
        st.session_state.score_hist.append(m["ai_trust_score"])
        st.session_state.prev_trust = clamp(0.6*st.session_state.prev_trust + 0.4*m["social_trust"], 0, 1)

        if st.button("ë‹¤ìŒ ë¼ìš´ë“œ â–¶"):
            st.session_state.round_idx += 1
            st.session_state.last_out = None
            st.rerun()

# ==================== Footer / Downloads ====================
st.markdown("---")
st.subheader("ğŸ“¥ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ")
if st.session_state.log:
    output = io.StringIO()
    writer = csv.DictWriter(output, fieldnames=list(st.session_state.log[0].keys()))
    writer.writeheader()
    writer.writerows(st.session_state.log)
    st.download_button(
        "CSV ë‚´ë ¤ë°›ê¸°",
        data=output.getvalue().encode("utf-8"),
        file_name="ethical_crossroads_log.csv",
        mime="text/csv"
    )

st.caption("â€» ë³¸ ì•±ì€ êµìœ¡Â·ì—°êµ¬ìš© ì‚¬ê³ ì‹¤í—˜ì…ë‹ˆë‹¤. ì‹¤ì œ ìœ„í•´ í–‰ìœ„ë‚˜ ì°¨ë³„ì„ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
